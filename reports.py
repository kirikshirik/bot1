# utils/reports.py
import logging
from datetime import datetime, timedelta, time
from collections import Counter, defaultdict
from pytz import timezone

from aiogram.utils.markdown import escape_md
from aiogram import Bot

from config import (SCHEDULER_TIMEZONE, TOP_N_REASONS_FOR_SUMMARY,
                    PRODUCTION_SITES, LINES_SECTIONS, ADMIN_ROLE)
from utils.storage import DataStorage

def get_shift_time_range(shift_type: str) -> (datetime, datetime):
    tz = timezone(SCHEDULER_TIMEZONE)
    now_local = datetime.now(tz)
    time_08_00 = time(8, 0)
    time_20_00 = time(20, 0)

    if time_08_00 <= now_local.time() < time_20_00:
        current_start = now_local.replace(hour=8, minute=0, second=0, microsecond=0)
        current_end = now_local.replace(hour=20, minute=0, second=0, microsecond=0)
    else:
        if now_local.time() >= time_20_00:
            current_start = now_local.replace(hour=20, minute=0, second=0, microsecond=0)
            current_end = (now_local + timedelta(days=1)).replace(hour=8, minute=0, second=0, microsecond=0)
        else:
            current_start = (now_local - timedelta(days=1)).replace(hour=20, minute=0, second=0, microsecond=0)
            current_end = now_local.replace(hour=8, minute=0, second=0, microsecond=0)

    if shift_type == 'current':
        return current_start, current_end
    elif shift_type == 'previous':
        if current_start.time() == time_08_00:
            prev_end = current_start
            prev_start = (current_start - timedelta(days=1)).replace(hour=20, minute=0, second=0, microsecond=0)
        else:
            prev_end = current_start
            prev_start = current_start.replace(hour=8, minute=0, second=0, microsecond=0)
        return prev_start, prev_end

    return None, None


def calculate_shift_times(record_datetime: datetime) -> (str, str):
    tz = timezone(SCHEDULER_TIMEZONE)
    record_datetime_aware = record_datetime.astimezone(tz) if record_datetime.tzinfo else tz.localize(record_datetime)
    record_date = record_datetime_aware.date()
    record_time = record_datetime_aware.time()
    time_08_00 = time(8, 0)
    time_20_00 = time(20, 0)

    if time_08_00 <= record_time < time_20_00:
        start_dt = tz.localize(datetime.combine(record_date, time_08_00))
        end_dt = tz.localize(datetime.combine(record_date, time_20_00))
    elif record_time >= time_20_00:
        start_dt = tz.localize(datetime.combine(record_date, time_20_00))
        end_dt = tz.localize(datetime.combine(record_date + timedelta(days=1), time_08_00))
    else:
        start_dt = tz.localize(datetime.combine(record_date - timedelta(days=1), time_20_00))
        end_dt = tz.localize(datetime.combine(record_date, time_08_00))

    return start_dt.strftime("%Y-%m-%d %H:%M:%S"), end_dt.strftime("%Y-%m-%d %H:%M:%S")

def _parse_datetime_from_sheet(dt_string: str) -> datetime | None:
    """–ü—ã—Ç–∞–µ—Ç—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Å—Ç—Ä–æ–∫—É —Å –¥–∞—Ç–æ–π –∏–∑ —Ç–∞–±–ª–∏—Ü—ã, –ø—Ä–æ–±—É—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–æ—Ä–º–∞—Ç–æ–≤."""
    # –°–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤, –æ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–æ–≥–æ –∫ –º–µ–Ω–µ–µ
    formats_to_try = [
        "%Y-%m-%d %H:%M:%S",  # –ù–∞—à –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç
        "%d.%m.%Y %H:%M:%S",  # –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –≤ –†–§
        "%Y/%m/%d %H:%M:%S",
    ]
    for fmt in formats_to_try:
        try:
            return datetime.strptime(dt_string, fmt)
        except ValueError:
            continue
    logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã-–≤—Ä–µ–º–µ–Ω–∏: '{dt_string}'")
    return None

async def get_downtime_report_for_period(start_dt: datetime, end_dt: datetime, storage: DataStorage):
    cache_status = ""
    if storage.downtime_cache.get("error"):
        cache_status += f"\n\n‚ö†Ô∏è **–ö—ç—à-–æ—à–∏–±–∫–∞: {storage.downtime_cache['error']}.**"
    if storage.is_cache_stale():
        cache_status += f"\n\n‚ö†Ô∏è **–î–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ–∞–∫—Ç—É–∞–ª—å–Ω—ã (–∫—ç—à —É—Å—Ç–∞—Ä–µ–ª).**"

    headers = storage.downtime_cache.get("headers")
    data_rows = storage.downtime_cache.get("data_rows")

    if not headers or data_rows is None:
        return f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ—Å—Ç–æ—è—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.{cache_status}"

    try:
        required_cols = [
            "Timestamp_–∑–∞–ø–∏—Å–∏", "–ü–ª–æ—â–∞–¥–∫–∞", "–õ–∏–Ω–∏—è_–°–µ–∫—Ü–∏—è", "–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ_–ø—Ä–æ—Å—Ç–æ—è",
            "–í—Ä–µ–º—è_–ø—Ä–æ—Å—Ç–æ—è_–º–∏–Ω—É—Ç", "–ü—Ä–∏—á–∏–Ω–∞_–ø—Ä–æ—Å—Ç–æ—è_–æ–ø–∏—Å–∞–Ω–∏–µ", "–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–∞—è_–≥—Ä—É–ø–ø–∞",
            "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π_–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π_–∏–Ω–∏—Ü–∏–∞—Ç–æ—Ä–∞"
        ]
        idx_map = {col: headers.index(col) for col in required_cols}
    except ValueError as e:
        logging.error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π —Å—Ç–æ–ª–±–µ—Ü –≤ —Ç–∞–±–ª–∏—Ü–µ: {e}")
        return f"–û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: —Å—Ç–æ–ª–±–µ—Ü '{str(e).split()[0]}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ç–∞–±–ª–∏—Ü–µ."

    downtimes_by_site = defaultdict(list)
    total_minutes = 0
    tz = timezone(SCHEDULER_TIMEZONE)

    for row in data_rows:
        try:
            if len(row) <= max(idx_map.values()): continue
            
            record_timestamp_str = row[idx_map["Timestamp_–∑–∞–ø–∏—Å–∏"]]
            if not record_timestamp_str: continue
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é –Ω–∞–¥–µ–∂–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            record_dt = _parse_datetime_from_sheet(record_timestamp_str)
            if not record_dt: continue

            record_dt_aware = tz.localize(record_dt)

            if start_dt <= record_dt_aware < end_dt:
                site = escape_md(row[idx_map['–ü–ª–æ—â–∞–¥–∫–∞']])
                duration = int(row[idx_map["–í—Ä–µ–º—è_–ø—Ä–æ—Å—Ç–æ—è_–º–∏–Ω—É—Ç"]] or 0)
                total_minutes += duration
                description = escape_md(row[idx_map["–ü—Ä–∏—á–∏–Ω–∞_–ø—Ä–æ—Å—Ç–æ—è_–æ–ø–∏—Å–∞–Ω–∏–µ"]])
                initiator_comment = escape_md(row[idx_map["–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π_–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π_–∏–Ω–∏—Ü–∏–∞—Ç–æ—Ä–∞"]])
                line_info = (f"‚öôÔ∏è **{escape_md(row[idx_map['–õ–∏–Ω–∏—è_–°–µ–∫—Ü–∏—è']])}**: "
                             f"{escape_md(row[idx_map['–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ_–ø—Ä–æ—Å—Ç–æ—è']])} ({duration} –º–∏–Ω.)\n"
                             f"   üìù _{description}_\n"
                             f"   üë• {escape_md(row[idx_map['–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–∞—è_–≥—Ä—É–ø–ø–∞']])}")

                if initiator_comment and "–ë–µ–∑ –¥–æ–ø. –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è" not in initiator_comment:
                    line_info += f"\n   üó£Ô∏è –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∏–Ω–∏—Ü–∏–∞—Ç–æ—Ä–∞: _{initiator_comment}_"
                downtimes_by_site[site].append(line_info)
        except (ValueError, IndexError) as e:
            logging.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞: {row}. –û—à–∏–±–∫–∞: {e}")
            continue

    if not downtimes_by_site:
        return f"–ù–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –∑–∞ —Å–º–µ–Ω—É —Å {start_dt.strftime('%H:%M %d.%m')} –ø–æ {end_dt.strftime('%H:%M %d.%m')}.{cache_status}"

    report_header = f"**üìä –û—Ç—á–µ—Ç –∑–∞ —Å–º–µ–Ω—É —Å {start_dt.strftime('%H:%M %d.%m')} –ø–æ {end_dt.strftime('%H:%M %d.%m')}**\n"
    report_lines = ["\n".join(lines) for site, lines in sorted(downtimes_by_site.items())]
    report_summary = f"\n\n**‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –ø—Ä–æ—Å—Ç–æ—è: {total_minutes} –º–∏–Ω—É—Ç.**"
    return report_header + "\n\n".join(report_lines) + report_summary + cache_status


async def generate_admin_shift_summary(start_dt: datetime, end_dt: datetime, storage: DataStorage):
    headers = storage.downtime_cache.get("headers")
    data_rows = storage.downtime_cache.get("data_rows")

    if not headers or data_rows is None: return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–≤–æ–¥–∫–∏."

    try:
        idx_map = {col: headers.index(col) for col in ["Timestamp_–∑–∞–ø–∏—Å–∏", "–í—Ä–µ–º—è_–ø—Ä–æ—Å—Ç–æ—è_–º–∏–Ω—É—Ç", "–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ_–ø—Ä–æ—Å—Ç–æ—è"]}
    except ValueError as e: return f"–û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–≤–æ–¥–∫–∏: —Å—Ç–æ–ª–±–µ—Ü '{str(e).split()[0]}' –Ω–µ –Ω–∞–π–¥–µ–Ω."
    
    total_minutes = 0
    reason_counts = Counter()
    tz = timezone(SCHEDULER_TIMEZONE)

    for row in data_rows:
        try:
            if len(row) <= max(idx_map.values()): continue
            record_timestamp_str = row[idx_map["Timestamp_–∑–∞–ø–∏—Å–∏"]]
            if not record_timestamp_str: continue
            
            record_dt = _parse_datetime_from_sheet(record_timestamp_str)
            if not record_dt: continue

            record_dt_aware = tz.localize(record_dt)

            if start_dt <= record_dt_aware < end_dt:
                duration = int(row[idx_map["–í—Ä–µ–º—è_–ø—Ä–æ—Å—Ç–æ—è_–º–∏–Ω—É—Ç"]] or 0)
                reason = row[idx_map["–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ_–ø—Ä–æ—Å—Ç–æ—è"]] or "–ù–µ —É–∫–∞–∑–∞–Ω–∞"
                total_minutes += duration
                reason_counts[reason] += duration
        except (ValueError, IndexError) as e:
            logging.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Å–≤–æ–¥–∫–∏: {row}. –û—à–∏–±–∫–∞: {e}")
            continue

    if total_minutes == 0:
        return f"–ó–∞ —Å–º–µ–Ω—É ({start_dt.strftime('%H:%M')}-{end_dt.strftime('%H:%M')}) –ø—Ä–æ—Å—Ç–æ–µ–≤ –Ω–µ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–æ."

    hours, minutes = divmod(total_minutes, 60)
    top_reasons_list = reason_counts.most_common(TOP_N_REASONS_FOR_SUMMARY)
    top_reasons = [f"- {escape_md(r)} ({m} –º–∏–Ω.)" for r, m in top_reasons_list]
    summary = (f"**–°–≤–æ–¥–∫–∞ –∑–∞ —Å–º–µ–Ω—É ({start_dt.strftime('%H:%M %d.%m')})**\n\n"
               f"–û–±—â–∏–π –ø—Ä–æ—Å—Ç–æ–π: **{hours} —á {minutes} –º–∏–Ω.**\n\n"
               f"**–¢–æ–ø-{len(top_reasons)} –ø—Ä–∏—á–∏–Ω—ã:**\n" + "\n".join(top_reasons))
    return summary


async def generate_line_status_report(storage: DataStorage):
    report_lines = ["**–°—Ç–∞—Ç—É—Å –ª–∏–Ω–∏–π –Ω–∞ —Ç–µ–∫—É—â–∏–π –º–æ–º–µ–Ω—Ç:**"]
    for site_key, site_name in PRODUCTION_SITES.items():
        if site_key not in LINES_SECTIONS: continue
        report_lines.append(f"\nüè≠ **{escape_md(site_name)}**")
        for line_key, line_name in LINES_SECTIONS[site_key].items():
            line_tuple = (site_name, line_name)
            if line_tuple in storage.active_downtimes:
                reason = storage.active_downtimes[line_tuple]
                report_lines.append(f"   üî¥ {escape_md(line_name)}: **–ü–†–û–°–¢–û–ô** ({escape_md(reason)})")
            else:
                report_lines.append(f"   üü¢ {escape_md(line_name)}: –†–∞–±–æ—Ç–∞–µ—Ç")
    return "\n".join(report_lines)


async def scheduled_line_status_report(bot: Bot, storage: DataStorage):
    logging.info("SCHEDULER: –ó–∞–ø—É—Å–∫ –∑–∞–¥–∞—á–∏ –Ω–∞ –æ—Ç–ø—Ä–∞–≤–∫—É –æ—Ç—á–µ—Ç–∞ –æ —Å—Ç–∞—Ç—É—Å–µ –ª–∏–Ω–∏–π.")
    admin_ids = [uid for uid, role in storage.user_roles.items() if role == ADMIN_ROLE]
    if not admin_ids:
        logging.warning("SCHEDULER: –ù–µ—Ç –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –æ—Ç—á–µ—Ç–∞ –æ —Å—Ç–∞—Ç—É—Å–µ –ª–∏–Ω–∏–π.")
        return
    report_text = await generate_line_status_report(storage)
    for admin_id in admin_ids:
        try:
            await bot.send_message(int(admin_id), report_text, parse_mode="Markdown")
        except Exception as e:
            logging.error(f"SCHEDULER: –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –æ—Ç—á–µ—Ç –æ —Å—Ç–∞—Ç—É—Å–µ –ª–∏–Ω–∏–π –∞–¥–º–∏–Ω—É {admin_id}: {e}")
    logging.info(f"SCHEDULER: –û—Ç—á–µ—Ç –æ —Å—Ç–∞—Ç—É—Å–µ –ª–∏–Ω–∏–π –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω {len(admin_ids)} –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞–º.")